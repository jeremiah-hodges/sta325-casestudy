---
title: "Case Study"
author: "Abdel Shehata"
date: "2022-10-26"
output:
  pdf_document:
    fig_height: 4
    fig_width: 6
fontsize: 7pt
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE, 
                      echo = FALSE,
                      fig.width = 6, #width of figure
                      fig.asp = .618, #set figure height based on aspect ratio
                      out.width = "50%", #width relative to text
                      fig.align = "center" #alignment
                      )
```

```{r, echo=FALSE, message=FALSE}
#| label: load-pkg-data
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(dplyr)
library(ggplot2)
library(cowplot)
library(knitr)
library(recipes)
library(caret)
library(InformationValue)
library(ISLR)
library(MASS)
library(nnet)
library(Stat2Data)
library(GGally)
library(leaps)
library(arm)
library(MASS)
library(stats)
library(car)
library(boot)
library(arm)
library(gam)
library(readr)
library(gridExtra)
library(grid)
library(ggplot2)
library(lattice)
```

## Introduction and Data

### Introduction



### Data Introduction




### Exploratory Data Analysis
```{r}
data_train <- read_csv("data-train.csv")

attach(data_train)
data_train <- data_train%>% mutate(TFr = case_when(Fr>1~ .99999, Fr<1~Fr))
data_train<-data_train%>%mutate(TFr=logit(TFr))

ggplot(data_train) +
  geom_histogram(aes(x = St), bins = 15)
```
```{r}
p1<-ggplot(data_train) +
  geom_histogram(aes(x = R_moment_1), bins = 10)
p2<-ggplot(data_train) +
  geom_histogram(aes(x = R_moment_2), bins = 10)
p3<-ggplot(data_train) +
  geom_histogram(aes(x = R_moment_3), bins = 10)
p4<-ggplot(data_train) +
  geom_histogram(aes(x = R_moment_4), bins = 10)
grid.arrange(p1, p2,p3,p4,nrow = 2)
```
```{r}

full_linear_E1 <- lm(R_moment_4 ~ Re+TFr, data = data_train)
```
Here are the linear models we obtained for the moments (without interactions):
\Large
<center>
$$ \hat{R_1}=0.0102+0.01353*St-0.0003798*Re$$
$$ \hat{R_2}=299.6593-0.8473*Re-10.2317TFr$$
$$ \hat{R_3}=2442265-6905*Re--83580TFr$$
$$ \hat{R_4}=2.008*10^{10}-5.677*10^{7}*Re-6.872*10^{8}TFr$$
<center>

\normalsize
```{r fig.height=5, fig.width=7}
full_linear_E1 <- lm(R_moment_1 ~ Re+St, data = data_train)
full_linear_E2 <- lm(R_moment_2~ Re+TFr, data = data_train)
full_linear_E3 <- lm(R_moment_3~ Re+TFr, data = data_train)
full_linear_E4 <- lm(R_moment_4 ~ Re+TFr, data = data_train)
par(mfrow=c(2,2))
p1<-plot(full_linear_E1,which=1)
p2<-plot(full_linear_E2,which=1)
p3<-plot(full_linear_E3,which=1)
p4<-plot(full_linear_E4,which=1)
```
```{r}
par(mfrow=c(2,2))
full_linear_interactions_E1 <- lm(R_moment_1 ~ St*TFr*Re, data = data_train)
step_full_linear_interactions_E1 <- stepAIC(full_linear_interactions_E1, direction = "both", trace = FALSE)
plot(step_full_linear_interactions_E1,which= 1:2)
```
```{r }
par(mfrow=c(2,2))
full_linear_interactions_E2 <- lm(R_moment_2 ~ St*TFr*Re, data = data_train)
step_full_linear_interactions_E2 <- stepAIC(full_linear_interactions_E2, direction = "both", trace = FALSE)
plot(step_full_linear_interactions_E2,which= 1)
full_linear_interactions_E3 <- lm(R_moment_3~ St*TFr*Re, data = data_train)
step_full_linear_interactions_E3 <- stepAIC(full_linear_interactions_E3, direction = "both", trace = FALSE)
plot(step_full_linear_interactions_E3,which= 1)

full_linear_interactions_E4 <- lm(R_moment_4~ St*TFr*Re, data = data_train)
step_full_linear_interactions_E4 <- stepAIC(full_linear_interactions_E4, direction = "both", trace= FALSE)

```
<center>
$$ \hat{R_1}=9.822*10^{-2}+3.398*10^{-2}*St-2.534*10^{-3}*TFr-3.176*10^{-4}*Re$$  $$-1.002*10^{-4}*St*Re+9.098*10^{-6}*TFr*Re$$
\par

$$ \hat{R_2}=327.50288 +46.54518*St-36.88062 *TFr-1.19146 *Re$$  $$-0.11802 *TFr*Re$$
\par
$$ \hat{R_3}=2525699.0 +556624.1*St-252310.1*TFr-9805.7*Re$$  $$-54692.6 *St*TFr+953.2*TFr*Re$$
$$ \hat{R_4}= 1.528*10^{10}+1.050*10^{10}*St-1.915*10^{10}*TFr-5.598*10^{7}*Re$$  $$-5.176*10^{8}*St*TFr-2.662*10^{7}*St*Re+7.7304*10^{6}*TFr*Re$$

### Model Evalutation 
```{r}
train.control <- trainControl(method = "LOOCV")
model1 <- train(R_moment_4 ~ St + TFr + Re + St:TFr + St:Re + TFr:Re,data = data_train,
                        method="lm",
                    trControl = train.control
                    )
```




|      Linear RMSE    | No Interactions | Interactions
|-----------|-------|----------|-------|
| **R_1**   |  0.0349    |  0.0340   |      
| **R_2**   |   237.411   | 222.3977 |   
| **R_3** |    1991432   |     1870992   |    
| **R_4** |    16757102688   |   15946689806  | 

|      Linear R-Squared    | No Interactions | Interactions
|-----------|-------|----------|-------|
| **R_1**   |  0.6054832  |  0.6282726   |      
| **R_2**   | 0.1716913    |0.2754936|   
| **R_3** |    0.161867   |     0.2650636  |    
| **R_4** |  0.1539926    | 0.2466392    | 



### Complex Model 

Lets First start start by evaluating a gam model to see if there is a complex relationship between our response variables and predictor variables. For this section of the project, I will be only employing the simpler lm function to keep all the models in the same format.
```{r}
gam1<-lm(R_moment_1~ns(TFr,2)+ns(Re,2)+ns(St,3),data=data_train)
gam2<-lm(R_moment_2~ns(TFr,2)+ns(Re,2)+ns(St,3),data=data_train)
gam3<-lm(R_moment_3~ns(TFr,2)+ns(Re,2)+ns(St,3),data=data_train)
gam4<-lm(R_moment_4~ns(TFr,2)+ns(Re,2)+ns(St,3),data=data_train)

Gam_Model<-c("gam1","gam2","gam3","gam4")
Adjusted.R.Squared<-c(summary(gam1)$adj.r.squared,summary(gam2)$adj.r.squared,summary(gam3)$adj.r.squared,summary(gam4)$adj.r.squared)
RSS<-c(anova(gam1)["Residuals", "Sum Sq"],anova(gam2)["Residuals", "Sum Sq"],anova(gam3)["Residuals", "Sum Sq"],anova(gam4)["Residuals", "Sum Sq"])
data1<-data.frame(Gam_Model,Adjusted.R.Squared,RSS)

knitr::kable(head(data1), "pipe")
```
As we can see, a Gam Model performs better with all the moments than linear regression. Nevertheless, as we can see the model only performs adequately with the first moment with an adjusted r square of 0.9244 and an RSS of 0.01536 compared to an average of 0.4 for the other models. This likely is due to the lack of interaction factors in our model, which appear to affect the second, third and fourth moment more than the first. Thus, we might need to add some interaction values to our polynomial model.


Note: 2 degrees were chosen due to the number of unique values of in our data. Only 3 degrees were chosen for St since it has multiple unique values.

##### Plots

Since the first GAM performed particulary well, it might be worthwhile to explore the relationship between our response varible and predicator varibles using plots.
```{r}
par(mfrow=c(1,3))
gam_one<- gam(R_moment_2~ns(TFr,2)+ns(Re,2)+ns(St,3),data=data_train)
plot(gam_one, se = TRUE, col = "blue")
```
As we can see from the plots, the first moment appears to experience a steep drop for both TFr and Re from the first to the second observation. The decrease becomes much less significant from the second observation to the third observation for both Tfr and Re. The relationship between St and the first moment appear to be roughly linear and increasing. 

#### Best Degree Model

We can utilize a sequential stepwise selection method with a full model. Our full model would utilize the max number of interactions with a polynomial degree of 2 for Tfr and Re (Max number of degrees based on unique values). We will use one for St since throughout our linear models and GAM models, it appears that the relationship between the St and the moments is approximetly linear.

```{r}
train.control <- trainControl(method = "LOOCV")
step.model.one <- train(R_moment_1~I(TFr^2)*I(Re^2)*St+St*Re*TFr, data = data_train,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax =1:13),
                    trControl = train.control
                    )
data1<-data.frame(step.model.one$results$nvmax, step.model.one$results$RMSE)
ggplot(data1,aes(x = step.model.one.results.nvmax, y =(step.model.one.results.RMSE)^2))+
         geom_line()+
         geom_point()+
  scale_x_continuous(breaks = 1:13, minor_breaks = NULL) +
   labs(title = "Training MSE based on Number of Variables ",
        x="Best Model with x Variables", y="MSE")
```
As we can see from the plot, there appears to be in MSE until about the 5 degrees best model. Then the MSE increases again until about the model with 9 variables, where it decreases again. Since, for the first moment, we are focusing more on inference and the error appears to be neigable between the fifth model and later models,we will choose the fifth model as our best model.
```{r}
step.model.two <- train(R_moment_2~I(TFr^2)*I(Re^2)*St+St*Re*TFr, data = data_train,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax =1:13),
                    trControl = train.control
)
data2<-data.frame(step.model.two$results$nvmax, step.model.two$results$RMSE)
ggplot(data2,aes(x = step.model.two.results.nvmax, y =(step.model.two.results.RMSE)^2))+
         geom_line()+
         geom_point()+
  scale_x_continuous(breaks = 1:13, minor_breaks = NULL) +
   labs(title = "Training MSE based on Number of Variables ",
        x="Best Model with x Variables", y="MSE")
```


As we can see from the plot, the model does best is the one with 12 variables. However, this model is most likely not very understandable since it does include a variety of interactions including the three way interaction. So the model with 9 variables might be better for some inference. Nevertheless, it does appear that the higher the moment the harder it is to predict.


```{r}
step.model.three <- train(R_moment_3~I(TFr^2)*I(Re^2)*St+St*Re*TFr, data = data_train,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax =1:13),
                    trControl = train.control
                    )
data2<-data.frame(step.model.three$results$nvmax, step.model.three$results$RMSE)
ggplot(data2,aes(x = step.model.three.results.nvmax, y =(step.model.three.results.RMSE)^2))+
         geom_line()+
         geom_point()+
  scale_x_continuous(breaks = 1:13, minor_breaks = NULL) +
   labs(title = "Training MSE based on Number of Variables ",
        x="Best Model with x Variables", y="MSE")



```
As we can see, the complex model (the almost full model with 12 variables) appears to be doing significantly best. However, since the errors are so big, it might be worth exploring the difference in error between including 12 variables and 8 variables in our final model.
```{r}
step.model.four<- train(R_moment_4~I(TFr^2)*I(Re^2)*St+St*Re*TFr, data = data_train,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax =1:13),
                    trControl = train.control
                    )
data2<-data.frame(step.model.four$results$nvmax, step.model.four$results$RMSE)
ggplot(data2,aes(x = step.model.four.results.nvmax, y =(step.model.four.results.RMSE)^2))+
         geom_line()+
         geom_point()+
  scale_x_continuous(breaks = 1:13, minor_breaks = NULL) +
   labs(title = "Training MSE based on Number of Variables ",
        x="Best Model with x Variables", y="MSE")

```
Interestingly, for the fourth moment, the full model appears to do the best. However, the model with 8 variables might of interest to explore since it might be better for inference while not scaificing a ton of accuracy.
\par




## Results (Final Models)
```{r}
model_1<-lm(R_moment_1~I(Re^2)+St+Re+I(Re^2)*St+St:Re,data=data_train)
summary(model_1)
```
As we can see a model with 5 variables is enough to predict the first raw moment accurately. Interestingly enough, similar to the linear models TFr doesn't appear to have a significant relationship between it and the first raw moment.

```{r}
model_2<-lm(R_moment_2~Re*TFr*St+I(TFr^2):I(Re^2)+I(TFr^2):St +I(TFr^2)+I(Re^2)-Re:TFr:St,data=data_train)
summary(model_2)
```
The final model for the second raw moment is a model with 10 variables that has up to two interaction levels.

```{r}
model_3<-lm(R_moment_3~I(TFr^2)*I(Re^2)+St*TFr+Re*TFr+I(TFr^2):St,data=data_train)
summary(model_3)
```
Our Final model for Moment 3 will be the full model with 13 variables (ST is in the model due to the hierarchy principle) . This model has all interaction variables up to 3, since they appear to be significant (P<0.05)

\footnotesize
```{r, out.width="50%"}
model_4<-lm(R_moment_4~I(TFr^2)*I(Re^2)+St*TFr+Re*TFr+I(TFr^2):St,data=data_train)
summary(model_4)
```
\normalsize
Our Final model for Moment 4 will be the full model with 13 variables (ST is in the model due to the hierarchy principle) . This model has all interaction variables up to 3, since they appear to be significant (P<0.05). 


## Discussion & Conclusion 



## References

