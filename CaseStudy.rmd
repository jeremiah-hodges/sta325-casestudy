---
title: "Case Study"
author: "Abdel Shehata"
date: "2022-10-26"
output: pdf_document
---

```{r, echo=FALSE, message=FALSE}
#| label: load-pkg-data
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(dplyr)
library(ggplot2)
library(cowplot)
library(knitr)
library(recipes)
library(caret)
library(InformationValue)
library(ISLR)
library(MASS)
library(nnet)
library(Stat2Data)
library(GGally)
library(leaps)
library(arm)
library(MASS)
library(stats)
library(car)
library(boot)
library(arm)
library(gam)
library(sjPlot)
```

## Introduction and Data

### Introduction



### Data Introduction




### Exploratory Data Analysis

```{r}
library(readr)
data_train <- read_csv("data-train.csv")

attach(data_train)
data_train <- data_train%>% mutate(TFr = case_when(Fr>1~ .99999, Fr<1~Fr))
data_train<-data_train%>%mutate(TFr=logit(TFr))

ggplot(data_train) +
  geom_histogram(aes(x = St), bins = 15)
ggplot(data_train) +
  geom_histogram(aes(x = Re), bins = 15)
ggplot(data_train) +
  geom_histogram(aes(x = Fr), bins = 15)
ggplot(data_train) +
  geom_histogram(aes(x = R_moment_1), bins = 15)
ggplot(data_train) +
  geom_histogram(aes(x = R_moment_2), bins = 15)
ggplot(data_train) +
  geom_histogram(aes(x = R_moment_3), bins = 15)
ggplot(data_train) +
  geom_histogram(aes(x = R_moment_4), bins = 15)

ggpairs(data_train)
```

Some brief notes:

Observations on predictors:
St (size) seems to be mostly small particles with some trials with larger particles.
Re (turbulence) seems to be in three groups: low (90), medium (224), and high (398). Perhaps it could be considered a categorical variable?
Fr (gravitational acceleration) seems to also be in three groups: low (.052), medium (.3), and high (infinite). Could this also become a categorical variable?
We have decided to do a logistic transformation on Fr in order to approximate the effects of infinity.

Also, I believe we should centralize the second through fourth moments. The first raw moment is actually helpful because it tells us about the average amount of turbulence. However, when it comes to the shape of the distribution (variance, skewness, and kurtosis) we need to centralize the moments in order to interpret them.

The code for transforming the variables is below:

```{r}

data_train <- data_train %>% mutate(R_moment_1_central = 0)
data_train <- data_train %>% mutate(R_moment_2_central = R_moment_2 - (R_moment_1)^2)
data_train <- data_train %>% mutate(R_moment_3_central = R_moment_3 - 3*R_moment_1*R_moment_2 + 2*(R_moment_1)^3)
data_train <- data_train %>% mutate(R_moment_4_central = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*((R_moment_1)^2)*R_moment_2 - 3*(R_moment_1)^4)
```



Correlations:
Reynolds number is negatively correlated with all moments, which is surprising but I believe it is due to the fact that almost all of the observations of all the moments are mostly around 0 with only some exceptions.
The 2nd, 3rd, and 4th moments are pretty correlated but this makes sense because they are all various measures of the width and shape of the tails.

Plots:
St and the first moment seem to have a linear or quadratic relationship. I would not be surprised if it is true that bigger particles cluster more on average.
St and the second, third, and fourth moments seem to have a linear or quadratic relationship. Perhaps bigger particles behave more unpredictably.


## Methodology

### Linear 


#### Linear Fitting

```{r}

full_linear_E1 <- glm(R_moment_1 ~ St + TFr + Re, data = data_train)
step_full_linear_E1 <- stepAIC(full_linear_E1, direction = "both", trace = FALSE)
summary(step_full_linear_E1)
plot(step_full_linear_E1)

full_linear_E2 <- glm(R_moment_2 ~ St + TFr + Re, data = data_train)
step_full_linear_E2 <- stepAIC(full_linear_E2, direction = "both", trace = FALSE)
summary(step_full_linear_E2)
plot(step_full_linear_E2)

full_linear_E3 <- glm(R_moment_3 ~ St + TFr + Re, data = data_train)
step_full_linear_E3 <- stepAIC(full_linear_E3, direction = "both", trace = FALSE)
summary(step_full_linear_E3)
plot(step_full_linear_E3)

full_linear_E4 <- glm(R_moment_4 ~ St + TFr + Re, data = data_train)
step_full_linear_E4 <- stepAIC(full_linear_E4, direction = "both", trace = FALSE)
summary(step_full_linear_E4)
plot(step_full_linear_E4)

```


#### Linear fitting on central moments 2 through 4

```{r}

full_linear_E2_central <- glm(R_moment_2_central ~ St + TFr + Re, data = data_train)
step_full_linear_E2_central <- stepAIC(full_linear_E2_central, direction = "both", trace = FALSE)
summary(step_full_linear_E2_central)
plot(step_full_linear_E2_central)

full_linear_E3_central <- glm(R_moment_3_central ~ St + TFr + Re, data = data_train)
step_full_linear_E3_central <- stepAIC(full_linear_E3_central, direction = "both", trace = FALSE)
summary(step_full_linear_E3_central)
plot(step_full_linear_E3_central)

full_linear_E4_central <- glm(R_moment_4_central ~ St + TFr + Re, data = data_train)
step_full_linear_E4_central <- stepAIC(full_linear_E4_central, direction = "both", trace = FALSE)
summary(step_full_linear_E4_central)
plot(step_full_linear_E4_central)
```


#### Best AiC model with interactions

```{r}
full_linear_interactions_E1 <- glm(R_moment_1 ~ St*TFr + St*Re + TFr*Re, data = data_train)
step_full_linear_interactions_E1 <- stepAIC(full_linear_interactions_E1, direction = "both", trace = FALSE)
summary(step_full_linear_interactions_E1)
plot(step_full_linear_interactions_E1)


full_linear_interactions_E2 <- glm(R_moment_2_central ~ St*TFr + St*Re + TFr*Re, data = data_train)
step_full_linear_interactions_E2 <- stepAIC(full_linear_interactions_E2, direction = "both", trace = FALSE)
summary(step_full_linear_interactions_E2)
plot(step_full_linear_interactions_E2)

full_linear_interactions_E3 <- glm(R_moment_3_central ~ St*TFr + St*Re + TFr*Re, data = data_train)
step_full_linear_interactions_E3 <- stepAIC(full_linear_interactions_E3, direction = "both", trace = FALSE)
summary(step_full_linear_interactions_E3)
plot(step_full_linear_interactions_E3)

full_linear_interactions_E4 <- glm(R_moment_4_central ~ St*TFr + St*Re + TFr*Re, data = data_train)
step_full_linear_interactions_E4 <- stepAIC(full_linear_interactions_E4, direction = "both", trace = FALSE)
summary(step_full_linear_interactions_E4)
plot(step_full_linear_interactions_E4)

```


#### Model Evaluation (Linear)

```{r}
library(boot)
cve_linear_E1 <- cv.glm(data_train, step_full_linear_E1, K=10)
cve_linear_E1$delta
cve_linear_interactions_E1 <- cv.glm(data_train, step_full_linear_interactions_E1, K = 10)
cve_linear_interactions_E1$delta


cve_linear_E2 <- cv.glm(data_train, step_full_linear_E2_central, K=10)
cve_linear_E2$delta
cve_linear_interactions_E2 <- cv.glm(data_train, step_full_linear_E2_central, K = 10)
cve_linear_interactions_E2$delta

cve_linear_E3 <- cv.glm(data_train, step_full_linear_E3_central, K=10)
cve_linear_E3$delta
cve_linear_interactions_E3 <- cv.glm(data_train, step_full_linear_interactions_E3, K = 10)
cve_linear_interactions_E3$delta

cve_linear_E4 <- cv.glm(data_train, step_full_linear_E4_central, K=10)
cve_linear_E4$delta
cve_linear_interactions_E4 <- cv.glm(data_train, step_full_linear_interactions_E4, K = 10)
cve_linear_interactions_E4$delta

```
It seems that the interactions are increasingly important. They are less important for the first and second moments. In fact, cross validation error increases for the second moment when interactions are added into the model. However, for the third through fourth moments, there is a pretty significant decrease in cross validation error when comparing the strictly linear models versus the ones with interactions.

This I think the linear models worth sharing with our physicist colleagues are the following:

```{r}
summary(step_full_linear_E1)

summary(step_full_linear_E2_central)

summary(step_full_linear_interactions_E3)

summary(step_full_linear_interactions_E4)

```
OR (by calling lm version of the functions)

```{r}
lm_fit_E1 <- lm(R_moment_1 ~ St + Re, data = data_train)
summary(lm_fit_E1)
plot(lm_fit_E1)
cve_linear_E1$delta
```
Surprisingly, a very simple linear model with only two out of the three predictors explains about 62% of the variation of the first moment. The Reynolds number coefficient is small and negative, which contradicts physics theory. I believe this is due to the fact that the overwhelming majority of observations had small mean turbulence, so the regression fit a line with negative slope. On average, we just do not often observe turbulence no matter what predictors are used. However, the coefficient on St is slightly larger and positive. I believe this shows that perhaps the most important contributor to increases in the first moment is the size of the particles. In fact, adding interactions or the Fr predictor did not change the R^2 very much, so I believe that St is very important for increasing average turbulence.
Nonetheless, there is a clear pattern to the residuals plot. First we underestimate, then overestimate, then underestimate again. This is evidence of a potential nonlinear relationship between the variables and the predictors.

```{r}
lm_fit_E2 <- lm(R_moment_2_central ~ TFr + Re, data = data_train)
summary(lm_fit_E2)
cve_linear_E2$delta
plot(lm_fit_E2)
```
The R^2 is quite low at only about 20%. Generally, I believe this linear model is not very helpful. There is another clear pattern in the residuals plot and the linear fit consistently underestimates when the second moment is large. The truth is definitely closer to a nonlinear relationship.

```{r}
lm_fit_E3 <- lm(R_moment_3_central ~ St + TFr + Re + St:TFr + TFr:Re, data = data_train)
summary(lm_fit_E3)
plot(lm_fit_E3)
cve_linear_interactions_E3$delta
```
The R^2 is still not great at only about 32%. However, the interaction terms give important theoretical insights that are more in line with the limited theory that we know. The coefficient on TFr:Re is positive, which means that even though the coefficients on Re and TFr alone are negative, we can infer that at high enough levels of TFr, the effect of Re will actually be positive (since the interaction term means that for a given TFr, the coefficient on Re is (-9805.2 + 953.2 * TFr)). Similarly, the effect of TFr will be positive at high enough levels of Re. Thus, increasing rightward skewness of the probability density functions with Re or TFr seems to occur only for a combination of high values of TFr and Re. Otherwise, the main positive coefficient is St. Again, we see that the size of the particles has a particularly straightforward effect on turbulence. It increases the first moment and the right skewness of the PDF.

```{r}
lm_fit_E4 <- lm(R_moment_4_central ~ St * TFr + St * Re + TFr * Re, data = data_train)
summary(lm_fit_E4)
plot(lm_fit_E4)
```

Our analysis of the best linear model for the fourth central moment is quite similar to that of the third central moment. St is the biggest positive driver of kurtosis in the PDF. TFr and Re individually are negative, but they have a positive interaction coefficient.


### Complex Model 

Lets First start start by evaluating a gam model to see if there is a complex relationship between our response variables and predictor variables. For this section of the project, I will be only employing the simpler lm function to keep all the models in the same format.
```{r,par(mfrow = c(2, 2))}
gam1<-lm(R_moment_1~ns(TFr,2)+ns(Re,2)+ns(St,3),data=data_train)
gam2<-lm(R_moment_2~ns(TFr,2)+ns(Re,2)+ns(St,3),data=data_train)
gam3<-lm(R_moment_3~ns(TFr,2)+ns(Re,2)+ns(St,3),data=data_train)
gam4<-lm(R_moment_4~ns(TFr,2)+ns(Re,2)+ns(St,3),data=data_train)
print(summary(gam1))
print(summary(gam2))
print(summary(gam3))
print(summary(gam4))
```
As we can see, a Gam Model performs better with all the moments than linear regression. Nevertheless, as we can see the model only performs adequately with the first moment with an adjusted r square of 0.9244 and an RSS of 0.01536 compared to an average of 0.4 for the other models. This likely is due to the lack of interaction factors in our model, which appear to affect the second, third and fourth moment more than the first. Thus, we might need to add some interaction values to our polynomial model.


Note: 2 degrees were chosen due to the number of unique values of in our data. Only 3 degrees were chosen for St since it has multiple unique values.

##### Plots

Since the first GAM performed particulary well, it might be worthwhile to explore the relationship between our response varible and predictor varibles using plots.
```{r}
par(mfrow=c(1,3))
gam_one<- gam(R_moment_1~ns(TFr,2)+ns(Re,2)+ns(St,3),data=data_train)
plot(gam_one, se = TRUE, col = "blue")
```
As we can see from the plots, the first moment appears to experience a steep drop for both TFr and Re from the first to the second observation. The decrease becomes much less significant from the second observation to the third observation for both Tfr and Re. The relationship between St and the first moment appear to be roughly linear and increasing. 

#### Best Degree Model

We can utilize a sequential stepwise selection method with a full model. Our full model would utilize the max number of interactions with a polynomial degree of 2 for TFr and Re (Max number of degrees based on unique values). We will use one for St since throughout our linear models and GAM models, it appears that the relationship between the St and the moments is approximately linear.

```{r}
train.control <- trainControl(method = "LOOCV")
step.model.one <- train(R_moment_1~I(TFr^2)*I(Re^2)*St+St*Re*TFr, data = data_train,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax =1:13),
                    trControl = train.control
                    )
step.model.one$results
coef(step.model.one$finalModel, 5)
data1<-data.frame(step.model.one$results$nvmax, step.model.one$results$RMSE)
ggplot(data1,aes(x = step.model.one.results.nvmax, y =(step.model.one.results.RMSE)^2))+
         geom_line()+
         geom_point()+
  scale_x_continuous(breaks = 1:13, minor_breaks = NULL) +
   labs(title = "Training MSE based on Number of Variables ",
        x="Best Model with x Variables", y="MSE")
```
As we can see from the plot, there appears to be a decrease in MSE until about the 5 degrees best model. Then the MSE increases again until about the model with 9 variables, where it decreases again. Since, for the first moment, we are focusing more on inference and the error appears to be negligible between the fifth model and later models, we will choose the fifth model as our best model.
\par
Next, we are going to repeat the same process for all the other moments with a focus on prediction instead of inference. 
```{r}
step.model.two <- train(R_moment_2~I(TFr^2)*I(Re^2)*St+St*Re*TFr, data = data_train,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax =1:13),
                    trControl = train.control
                    )
step.model.two$results
```
```{r}
data2<-data.frame(step.model.two$results$nvmax, step.model.two$results$RMSE)
ggplot(data2,aes(x = step.model.two.results.nvmax, y =(step.model.two.results.RMSE)^2))+
         geom_line()+
         geom_point()+
  scale_x_continuous(breaks = 1:13, minor_breaks = NULL) +
   labs(title = "Training MSE based on Number of Variables ",
        x="Best Model with x Variables", y="MSE")
```
As we can see from the plot, the model does best is the one with 12 variables. However, this model is most likely not very understandable since it does include a variety of interactions including the three way interaction. So the model with 9 variables might be better for some inference. [Francis: I would prefer to go with 6 because even that is a lot of variables already.] Nevertheless, it does appear that the higher the moment the harder it is to predict.




```{r}
step.model.three <- train(R_moment_3~I(TFr^2)*I(Re^2)*St+St*Re*TFr, data = data_train,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax =1:13),
                    trControl = train.control
                    )
step.model.three$results
data2<-data.frame(step.model.three$results$nvmax, step.model.three$results$RMSE)
ggplot(data2,aes(x = step.model.three.results.nvmax, y =(step.model.three.results.RMSE)^2))+
         geom_line()+
         geom_point()+
  scale_x_continuous(breaks = 1:13, minor_breaks = NULL) +
   labs(title = "Training MSE based on Number of Variables ",
        x="Best Model with x Variables", y="MSE")



```
As we can see, the complex model (the almost full model with 12 variables) appears to be doing significantly best. However, since the errors are so big, it might be worth exploring the difference in error between including 12 variables and 8 variables in our final model. [Francis: 6 looks fine again]
```{r}
step.model.four<- train(R_moment_4~I(TFr^2)*I(Re^2)*St+St*Re*TFr, data = data_train,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax =1:13),
                    trControl = train.control
                    )
step.model.four$results
data2<-data.frame(step.model.four$results$nvmax, step.model.four$results$RMSE)
ggplot(data2,aes(x = step.model.four.results.nvmax, y =(step.model.four.results.RMSE)^2))+
         geom_line()+
         geom_point()+
  scale_x_continuous(breaks = 1:13, minor_breaks = NULL) +
   labs(title = "Training MSE based on Number of Variables ",
        x="Best Model with x Variables", y="MSE")

```
Interestingly, for the fourth moment, the full model appears to do the best. However, the model with 8 variables might of interest to explore since it might be better for inference while not sacrificing a ton of accuracy. [Francis: That's a good idea. Although for kurtosis (the fourth moment) we really aren't learning much about the distribution to begin with so there is no harm in aiming for good prediction and having a flexible model with 6 or 10 variables. I think it might be worth trying 4 variables for the other models 2 & 3 instead, so we can do more single variable plots.]
\par




## Results (Final Models)
```{r}
model_1<-lm(R_moment_1~I(Re^2)+St+Re+I(Re^2)*St+St:Re,data=data_train)
summary(model_1)
```
As we can see a model with 5 variables is enough to predict the first raw moment accurately. Interestingly enough, similar to the linear models TFr doesn't appear to have a significant relationship between it and the first raw moment.

```{r}
plot_model(model_1, type = "pred", terms = c("St", "Re[90,224,398]"))
plot_model(model_1, type = "pred", terms = c("Re","St[.05,.7,1.4]"))

```
This plot of the effect of Re on the first moment using the nonlinear model with interactions sheds light on the relationship between Re and the first moment. First, as we determined with the simple linear model, St has a positive relationship with the first moment. The impact of Re on the first moment is higher at each level of Re if St increases. However, this plot shows that as Re grows beyond roughly 50000, its marginal influence on the first moment increases. Thus, at very high values R seems to have a nonlinear relationship to the average expected amount of clustering.


```{r}
model_2<-lm(R_moment_2~Re*TFr*St+I(TFr^2):I(Re^2)+I(TFr^2):St +I(TFr^2)+I(Re^2)-Re:TFr:St,data=data_train)
summary(model_2)
```
The final model for the second raw moment is a model with 10 variables that has up to two interaction levels.

```{r}
plot_model(model_2, type = "pred", terms = c("TFr", "St[.05,.7,1.4]","Re[90,224,398]")) ##checking 500 is interesting
plot_model(model_2, type = "pred", terms = c("St", "TFr[-.84,-2.9,11.5]", "Re[90,224,398]"))
plot_model(model_2, type = "pred", terms = c("Re", "St[.05,.7,1.4]", "TFr[-.84,-2.9,11.5]"))
```
Third plot series:
From these plots of the second model, we can first notice a significant amount of overlap of error bounds in all three plots regardless of the level of St. Thus, when it comes to variance, the influence of the size of the particles is not particularly important. We can also see that the convergence of the fits tightens at TFr = 11.5 (Fr = inf). Because Fr = u/sqrt(gL), it makes sense that u (flow velocity) must be relatively large for Fr to be infinite (find any source). At high rates of flow may consistently break up the clusters and thus preventing the occasional examples of high levels of clustering. We also see that high Fr may limit the influence of Re. At near-zero Fr (TFr = -.84), high Re has a strong positive relationship with variance. Additionally, Re seems to exhibit some kind of thresholding behavior past Re = 224 where its relationship with variance reverses past this point. Re = 224 appears to be a special value worth studying more in the future.

Second plot series:
These plots show us a relationship that later holds true in the second and third moments as well. First, we can see by the blue and green lines that regardless of the level of Re and St, higher levels of Fr decrease the variance of clustering. We saw this relationship in the previous set of plots. The most interesting line to observe is the red one that shows what happens to variance as St changes at near-zero Fr. As Re increases, the effect of St converges to essentially nothing. Combined with what we learn about the first moment (that at extremely high levels of Re, average clustering increases), perhaps this means that Re becomes the most important determinant of clustering as it reaches high levels. At very high Re, we may see consistent clustering regardless of the other variables. Nonetheless, if Re is low (90 or 224), St contributes to increases in variance. Thus, at low Re and Fr, not only does St increase average clustering (as we observed in the linear models), it does so with a great deal of variance. This probably reflects the fact that particles colliding does increase in frequency as they grow in size, but collisions are very unpredictable and random events; sometimes they lead to clustering and other times they do not.


```{r}
coef(step.model.two$finalModel, 9)
model_2_alt <- lm(R_moment_2~I(TFr^2)*I(Re^2) + I(TFr^2):St + St:Re + St:TFr + Re*TFr, data=data_train)
summary(model_2_alt)
plot_model(model_2_alt, type = "pred", terms = c("TFr", "St[.05,.7,1.4]","Re[90,224,398]"))
plot_model(model_2_alt, type = "pred", terms = c("St", "TFr[-.84,-2.9,11.5]", "Re[90,224,398]"))
plot_model(model_2_alt, type = "pred", terms = c("Re", "St[.05,.7,1.4]", "TFr[-.84,-2.9,11.5]"))


```


```{r}
model_3<-lm(R_moment_3~I(TFr^2)*I(Re^2)+St*TFr+Re*TFr+I(TFr^2):St,data=data_train)
summary(model_3)
```

```{r}
plot_model(model_3, type = "pred", terms = c("TFr", "St[.05,.7,1.4]","Re[90,224,398]"))
plot_model(model_3, type = "pred", terms = c("St", "TFr[-.84,-2.9,11.5]", "Re[90,224,398]"))
plot_model(model_3, type = "pred", terms = c("Re", "St[.05,.7,1.4]", "TFr[-.84,-2.9,11.5]"))
```
Second plot:
Once again, we see essentially the same relationship between St and the third moment as we do between St and the second moment. Thus, higher Fr and higher Re decreases skewness. However, the slope of the red lines is always positive. Thus, in this case, St appears to lengthen the right handed tail even as variance decreases due to Fr and Re. Thus, we can see that in a limited way, particle size increases the chances of rare high-clustering events within certain fixed conditions (where variance is limited overall by Fr and Re).

```{r}
coef(step.model.three$finalModel, 10)
model_3_alt <- lm(R_moment_3~I(TFr^2)+I(Re^2)+Re+TFr+I(TFr^2):I(Re^2)+I(TFr^2):St+St:Re+St:TFr+Re:TFr+St:Re:TFr, data=data_train)
summary(model_3_alt)
plot_model(model_3_alt, type = "pred", terms = c("TFr", "St[.05,.7,1.4]","Re[90,224,398]"))
plot_model(model_3_alt, type = "pred", terms = c("St", "TFr[-.84,-2.9,11.5]", "Re[90,224,398]"))
plot_model(model_3_alt, type = "pred", terms = c("Re", "St[.05,.7,1.4]", "TFr[-.84,-2.9,11.5]"))

```


Our Final model for Moment 4 will be the full model with 13 variables (ST is in the model due to the hierarchy principle) . This model has all interaction variables up to 3, since they appear to be significant (P<0.05)

```{r}
model_4<-lm(R_moment_4~I(TFr^2)*I(Re^2)+St*TFr+Re*TFr+I(TFr^2):St,data=data_train)
summary(model_4)
```
Our Final model for Moment 4 will be the full model with 13 variables (ST is in the model due to the hierarchy principle) . This model has all interaction variables up to 3, since they appear to be significant (P<0.05). 

```{r}
plot_model(model_4, type = "pred", terms = c("TFr", "St[.05,.7,1.4]","Re[90,224,398]"))
plot_model(model_4, type = "pred", terms = c("St", "TFr[-.84,-2.9,11.5]", "Re[90,224,398]"))
plot_model(model_4, type = "pred", terms = c("Re", "St[.05,.7,1.4]", "TFr[-.84,-2.9,11.5]"))
```
Second plot series:
The relationship between the predictors and kurtosis essentially seems identical to their relationship with skewness. Thus, Re = 398 plot is interesting because in this plot for the second moment, we saw the effects of all variables converge to zero. Thus, even as variance is not very effected, the size of the particles still makes the tails of the probability distribution of clustering heavier. Similar to our analysis of St's relationship to the third moment, it seems to increase the spread of the distribution within bounds set by the other parameters. Thus, as the other parameters make tail weight smaller overall, higher St makes them as small as possible within those bounds. I believe that this makes sense because Fr and Re seem to be more related to the environment than St, which has to do with the particles themselves. Given certain environmental parameters, larger particle size will always increase average clustering but increase left-leaning skewness and tail weight within the limited variance determined by Re and Fr.

## Discussion & Conclusion 

Conclusion:
In conclusion, we have learned that St and the first moment have a mostly positive and linear relationship, although predictive accuracy can increase if we use a more complex model that includes interactions. Through the model with interactions, we saw that extremely high values of Re will increase average clustering. The other moments are best fitted with models that involve complex interactions and exponential terms due to their inherent nonlinearity. In general, high levels of Fr and Re decrease variance, skewness, and kurtosis. These parameters seem to make the results of the simulation more regular and consistent. On the other hand, St pushes for more positive skewness and kurtosis. I believe that this probably has to do with the inherently chaotic and unpredictable nature of collisions between particles. Lastly, with regards to variance, we saw Re = 224 exhibits some kind of thresholding behavior because its effect on variance switches at that point. In addition to this mystery, the specific relationship between Re and Fr is also worth further consideration. Intuitively, as Fr increases I would expect clustering behavior to become more regular because high flow probably cuts short any particularly unusual behavior of particles. However, Re when combined with Fr also seems to concentrate the probability of clustering around the mean. Theoretically, I would expect Re to both increase mean clustering and lead to less predictable behavior. Perhaps our results mean that high levels of Re in fact leads to more clustering, albeit in very consistent ways.

## References
https://www.britannica.com/science/Reynolds-number

## Smaller Models

```{r}
coef(step.model.two$finalModel, 6)
```
```{r}
model_2_small <- lm(R_moment_2~I(TFr^2)*I(Re^2)+Re*TFr,data=data_train)
summary(model_2_small)
```
```{r}
plot_model(model_2_small, type = "pred", terms = c("TFr", "Re[90,224,398]"))
```



```{r}
coef(step.model.three$finalModel, 6)
```

```{r}
model_3_small <- lm(R_moment_3~I(TFr^2)*I(Re^2)+Re*TFr,data=data_train)
summary(model_3_small)
```
```{r}
plot_model(model_3_small, type = "pred", terms = c("TFr", "Re[90,224,398]"))
```

```{r}
coef(step.model.four$finalModel, 6)
```
```{r}
model_4_small <- lm(R_moment_4~I(TFr^2)*I(Re^2)+Re*TFr,data=data_train)
summary(model_4_small)
```
```{r}
plot_model(model_4_small, type = "pred", terms = c("TFr", "Re[90,224,398]"))
```
```

